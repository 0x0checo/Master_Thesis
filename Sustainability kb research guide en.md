# Sustainability Knowledge Base Research Project Guide

## ðŸ“‹ Table of Contents
1. [Project Overview](#project-overview)
2. [Four Research Questions Explained](#four-research-questions-explained)
3. [Knowledge Base Core Concepts](#knowledge-base-core-concepts)
4. [Comparison of Three Representations](#comparison-of-three-representations)
5. [API and Interface Design](#api-and-interface-design)
6. [Evaluation Metrics Framework](#evaluation-metrics-framework)
7. [Implementation Path and Timeline](#implementation-path-and-timeline)

---

## Project Overview

### Core Objective
Build a **Sustainability Knowledge Base** to support AI-assisted discussion moderation (moderator bot).

### Research Framework
```
PDF/Literature â†’ LLM Extraction â†’ Structured Representation â†’ Unified API â†’ Downstream RAG Application
                                            â†“
                            Compare effectiveness of different representations
```

### Four Research Questions
| RQ | Core Question | Focus |
|----|---------------|-------|
| RQ1 | How to extract and transform sustainability principles? | Data Preparation (Non-core) |
| RQ2 | Compare different representation formats | **Core Contribution** |
| RQ3 | Which is most suitable for discussion moderation scenario? | **Core Contribution** |
| RQ4 | What are the evaluation criteria? | Methodology |

---

## Four Research Questions Explained

### RQ1: Data Extraction and Transformation

> **Recommendation**: No need to compare multiple extraction methods; LLM extraction is sufficient

#### Revised RQ1 Statement
~~"How can sustainability principles be reliably extracted..."~~

â†’ "How can sustainability principles from research literature be **transformed into structured representations** suitable for reasoning?"

#### Specific Steps
1. **Use LLM for information extraction** (single method)
   - Use GPT/Claude + prompt engineering
   - Output structured JSON

2. **Demonstrate transformation pipeline**
   ```
   PDF/Text â†’ LLM extraction â†’ Raw JSON 
                                  â”œâ”€> Schema-based (JSON/SQL)
                                  â”œâ”€> Knowledge Graph (Neo4j)  
                                  â””â”€> Ontology (optional)
   ```

3. **Validate extraction quality**
   - Manual inspection of sample (50-100 principles)
   - Report Precision, Recall (to prove data quality is reliable)

#### Thesis Space Allocation
- Extraction: 1 chapter (Data preparation in Methodology)
- Representation comparison: 2-3 chapters (**Core!**)

---

### RQ2: Representation Comparison (Core)

#### Recommended Formats for Comparison

| Representation | Tool | Advantages | Disadvantages |
|----------------|------|------------|---------------|
| **JSON** | Text file | Simple, easy to understand | Slow queries, difficult reasoning |
| **SQL Database** | SQLite | Fast queries, clear structure | Inflexible relationship representation |
| **Knowledge Graph** | Neo4j | Complex reasoning, strong relationship expression | Steep learning curve |

#### Recommended Combinations
- **Option A (Simplest)**: JSON vs Knowledge Graph
- **Option B (Recommended)**: SQL vs Knowledge Graph
- **Option C (Complete)**: JSON + SQL + Knowledge Graph

#### Specific Representation Examples

**Schema-based (JSON)**
```json
{
  "principle_id": "P001",
  "principle_text": "Avoid systematic increases of...",
  "domain": "energy",
  "topic": "wind_energy",
  "phase": "production",
  "risks": ["bird_mortality", "habitat_disruption"],
  "mitigations": ["site_selection", "turbine_design"],
  "sources": [{"doc": "Smith2023.pdf", "page": 5}]
}
```

**Knowledge Graph (RDF triplets)**
```
(WindEnergy)--[IN_DOMAIN]-->(Energy)
(WindEnergy)--[HAS_RISK]-->(BirdMortality)
(BirdMortality)--[MITIGATED_BY]-->(SiteSelection)
(P001)--[CITED_FROM]-->(Source:Smith2023)
```

---

### RQ3: Moderation Scenario Evaluation (Core)

#### Simulation Scenario
Assume during discussion: *"we're using wind energy for our factory"*

Moderator needs to query:
- What sustainability risks exist?
- What questions need to be asked?
- Can the reasoning process be explained?

#### Testing Dimensions
1. **Query speed** - Does it meet real-time requirements?
2. **Generation quality** - Can it produce meaningful intervention questions?
3. **Explainability** - Can it explain the reasoning process?

#### User Study (if time permits)
- Show sustainability experts questions generated by different representations
- Have them evaluate quality

---

### RQ4: Evaluation Methodology

Answer during RQ1-3 process, see [Evaluation Metrics Framework](#evaluation-metrics-framework)

---

## Knowledge Base Core Concepts

### What is a Knowledge Base?

```
Knowledge Base = Data Content + Storage Format + Query Interface
```

#### Analogy for Understanding
- ðŸ“š **Library** = Knowledge Base
- ðŸ“– **Book** = Each sustainability principle
- ðŸ—‚ï¸ **Classification System** = Different representation methods

### Physical Form of Knowledge Base

| Representation | What is the KB | Is it a JSON file? | Format for Downstream |
|----------------|----------------|--------------------|-----------------------|
| Pure JSON | `kb.json` | âœ… Yes | Same file |
| SQL Database | `kb.db` (SQLite) | âŒ No | export.json |
| Knowledge Graph | `neo4j_data/` (folder) | âŒ No | export.json |

### Unified Knowledge Base Design (Recommended)

**Don't**: Create independent knowledge base for each domain  
**Should**: One knowledge base, use metadata to distinguish domains

```json
{
  "principle_id": "P001",
  "principle_text": "...",
  "fssd_principle": "principle_1",
  "domain": "energy",           // energy / transport / chemicals
  "topic": "wind_energy",
  "subtopic": "offshore_wind",
  "lifecycle_phase": "production",  // supply / production / use
  "risks": [...],
  "mitigations": [...],
  "sources": [...]
}
```

#### Advantages
1. **Cross-domain reasoning**: Can reason across domains (e.g., electric transport involves energy + transport)
2. **Shared principles**: 8 FSSD principles are universal
3. **Easier maintenance**: Maintain one system
4. **Better comparison**: More fair when comparing in RQ2 with unified data

---

## Comparison of Three Representations

### Format A: Pure JSON (Simplest)

```python
# knowledge_base_json.py
class JSONKnowledgeBase:
    def __init__(self, file_path):
        with open(file_path) as f:
            self.data = json.load(f)
    
    def query(self, domain=None, topic=None):
        results = self.data
        if domain:
            results = [p for p in results if p['domain'] == domain]
        if topic:
            results = [p for p in results if p['topic'] == topic]
        return results
```

**Pros**: Super simple  
**Cons**: Slow queries (with large data), cannot do complex reasoning

---

### Format B: SQL Database

```sql
CREATE TABLE principles (
    id TEXT PRIMARY KEY,
    text TEXT,
    domain TEXT,
    topic TEXT,
    phase TEXT
);

CREATE TABLE risks (
    principle_id TEXT,
    risk TEXT,
    FOREIGN KEY (principle_id) REFERENCES principles(id)
);

-- Query example
SELECT risk FROM principles 
JOIN risks ON principles.id = risks.principle_id
WHERE domain = 'energy';
```

**Pros**: Fast queries, clear structure  
**Cons**: Relationship representation not flexible enough

---

### Format C: Knowledge Graph (Neo4j)

```cypher
// Create nodes and relationships
CREATE (p:Principle {id: 'P001', text: '...'})
CREATE (d:Domain {name: 'energy'})
CREATE (t:Topic {name: 'wind_energy'})
CREATE (r:Risk {name: 'bird_mortality'})

CREATE (p)-[:IN_DOMAIN]->(d)
CREATE (p)-[:ABOUT]->(t)
CREATE (p)-[:HAS_RISK]->(r)

// Query example
MATCH (d:Domain {name:'energy'})<-[:IN_DOMAIN]-(p:Principle)-[:HAS_RISK]->(r:Risk)
RETURN p.text, r.name
```

**Pros**: Excels at complex relationships, strong reasoning capability  
**Cons**: Steep learning curve

---

## API and Interface Design

### Why is an API Needed?

```
Your work: Knowledge Base â†’ via API â†’ Downstream colleague's RAG Bot
```

Analogy: API is like a "telephone system" that specifies "how to ask" and "how to answer"

### Comparison of Three Approaches

| Approach | Difficulty | Time | Applicable Stage |
|----------|------------|------|------------------|
| **Approach 1: JSON Files** | â­ | 1 day | Week 1-6 |
| **Approach 2: Python Module** | â­â­ | 1-2 days | Week 5-8 |
| **Approach 3: REST API** | â­â­â­ | 3-7 days | Week 12+ |

---

### Approach 1: JSON File Exchange (Recommended to Start)

```python
# export_kb.py
def export_knowledge_base():
    # Export from database
    data = export_from_sql()  # or export_from_neo4j()
    
    with open("knowledge_base.json", "w") as f:
        json.dump(data, f, indent=2)

# Downstream colleague usage
with open("knowledge_base.json") as f:
    kb = json.load(f)
    results = [p for p in kb if p["topic"] == "wind_energy"]
```

---

### Approach 2: Python Module Interface

```python
# knowledge_base.py
class KnowledgeBase:
    def __init__(self, representation_type="schema"):
        self.type = representation_type
        # Initialize connection...
    
    def query(self, topic=None, domain=None, phase=None):
        """Unified query interface"""
        if self.type == "schema":
            return self._query_schema(topic, domain, phase)
        elif self.type == "graph":
            return self._query_graph(topic, domain, phase)
    
    def get_risks(self, topic):
        """Convenience method"""
        results = self.query(topic=topic)
        return list(set(r for p in results for r in p["risks"]))

# Downstream colleague usage
from knowledge_base import KnowledgeBase
kb = KnowledgeBase(representation_type="schema")
risks = kb.get_risks("wind_energy")
```

---

### Approach 3: REST API

```python
# api_server.py
from flask import Flask, request, jsonify

app = Flask(__name__)

@app.route('/api/query', methods=['GET'])
def query():
    topic = request.args.get('topic')
    domain = request.args.get('domain')
    representation = request.args.get('representation', 'schema')
    
    kb = kb_schema if representation == "schema" else kb_graph
    results = kb.query(topic=topic, domain=domain)
    
    return jsonify({"results": results, "count": len(results)})

# Run: python api_server.py
# Access: GET /api/query?topic=wind_energy&representation=schema
```

---

### Deployment Options

| Option | Cost | Use Case |
|--------|------|----------|
| **Local Computer** | Free | Development testing, same office collaboration |
| **Company Server** | Free | Requires IT assistance for deployment |
| **Cloud (Render)** | Free tier | Thesis demo, remote access |

**Recommended Path**:
- Week 1-8: Local development
- Week 12+: If demo needed, deploy to Render (1 day, free)

---

## Evaluation Metrics Framework

### 1. Traceability

> Can structured facts be traced back to original literature?

| Metric | Calculation Method | Target |
|--------|-------------------|--------|
| Source Coverage Rate | Facts with source / Total facts | 100% |
| Source Retrieval Accuracy | Correct references / Sample size | >90% |
| Tracing Granularity | Level 1-3 | Level 3 |

**Granularity Levels**:
- Level 1: Only know which paper it comes from
- Level 2: Know page number
- Level 3: Know exact sentence/paragraph

**Implementation Requirement**:
```json
{
  "source_document": "paper_id",
  "source_location": "page 5, section 2.3",
  "original_text": "exact quote from source"
}
```

---

### 2. Reasoning Capability

> Can logical reasoning be performed based on facts?

#### Reasoning Tasks

| Task Type | Example | Difficulty |
|-----------|---------|------------|
| Simple Lookup | "What are the risks of wind energy?" | â­ |
| Multi-hop | "If we use wind energy, what principles might be violated?" | â­â­ |
| Contradiction Detection | Detect contradictory facts | â­â­â­ |
| Explanation Generation | "Why is chemical X problematic?" | â­â­â­ |

#### Evaluation Metrics

| Metric | Description |
|--------|-------------|
| Reasoning Coverage | How many task types are supported |
| Inference Correctness | Correct answers / Total queries |
| Reasoning Depth | Average reasoning steps |
| Execution Time | Time per query (ms) |

---

### 3. Ease of Maintenance

> Can subject matter experts easily maintain the KB?

| Metric | Method | 
|--------|--------|
| Human Readability Score | Expert rating 1-5 |
| Edit Complexity | Number of steps required for modification |
| Error Rate | Number of errors in user tasks |
| Task Completion Time | Time to complete CRUD operations |

#### User Study Design
1. Recruit 3-5 sustainability experts
2. Have them complete the following tasks:
   - Task 1: "Find all facts about wind energy"
   - Task 2: "Correct this wrong fact"
   - Task 3: "Add a new mitigation strategy"
3. Record: Completion time, error count, subjective satisfaction

---

## Implementation Path and Timeline

### Recommended Schedule

| Phase | Weeks | Tasks |
|-------|-------|-------|
| **Phase 1** | Week 1-2 | Implement first representation with JSON |
| **Phase 2** | Week 3-4 | Learn and implement Knowledge Graph |
| **Phase 3** | Week 5-6 | Comparative evaluation of two representations |
| **Phase 4** | Week 7-8 | Upgrade to Python module interface |
| **Phase 5** | Week 9-10 | Moderation scenario testing |
| **Phase 6** | Week 11-12 | User Study (if time permits) |
| **Phase 7** | Week 12+ | Deploy to Cloud (optional) |

### Project File Structure

```
ðŸ“ your_project/
â”œâ”€â”€ ðŸ“„ data/
â”‚   â”œâ”€â”€ raw_papers/              # Original PDF files
â”‚   â””â”€â”€ extracted/               # Raw data extracted by LLM
â”‚       â””â”€â”€ extracted_facts.json
â”‚
â”œâ”€â”€ ðŸ“Š representations/          # Knowledge bases in different formats
â”‚   â”œâ”€â”€ json_based/
â”‚   â”‚   â””â”€â”€ kb.json              # JSON knowledge base
â”‚   â”œâ”€â”€ schema_based/
â”‚   â”‚   â”œâ”€â”€ kb.db                # SQLite database
â”‚   â”‚   â””â”€â”€ export.json
â”‚   â””â”€â”€ knowledge_graph/
â”‚       â”œâ”€â”€ neo4j_data/          # Neo4j data
â”‚       â””â”€â”€ export.json
â”‚
â”œâ”€â”€ ðŸ“ code/
â”‚   â”œâ”€â”€ extraction.py            # LLM extraction pipeline
â”‚   â”œâ”€â”€ build_json_kb.py         # Build JSON KB
â”‚   â”œâ”€â”€ build_schema_kb.py       # Build SQL KB
â”‚   â”œâ”€â”€ build_graph_kb.py        # Build Knowledge Graph
â”‚   â”œâ”€â”€ knowledge_base.py        # Unified query interface
â”‚   â”œâ”€â”€ evaluation.py            # Evaluation scripts
â”‚   â””â”€â”€ api_server.py            # API service
â”‚
â”œâ”€â”€ ðŸ“‹ evaluation/
â”‚   â”œâ”€â”€ test_queries.json        # Test queries
â”‚   â”œâ”€â”€ gold_standard.json       # Ground truth
â”‚   â””â”€â”€ results/                 # Evaluation results
â”‚
â””â”€â”€ ðŸ“„ README.md
```

---

## Coordination Checklist with Downstream Colleagues

### Week 1-2: Determine Interface

Send email to confirm:
1. What output format is needed? Is JSON sufficient?
2. What information needs to be queried? (By topic/domain/risk?)
3. What fields are needed? (Risks/Mitigations/Sources?)
4. Response time requirements?
5. Can we start with JSON files?

### Week 3: Provide First Version of Data

Deliverables:
- `knowledge_base_schema.json`
- `knowledge_base_graph.json`
- `README.md`

### Week 8: Collect Feedback

Collect actual queries from downstream colleagues as evaluation benchmarks

---

## Thesis Writing Suggestions

### About Extraction (RQ1)
> "We employ LLM-based extraction using GPT-4/Claude with carefully designed prompts. Manual verification of a sample of 100 principles showed precision of X% and recall of Y%, demonstrating sufficient data quality for our comparison study."

### About Representation (RQ2)
> "We compare two representation approaches: (1) Schema-based representation using structured JSON/SQL, and (2) Graph-based representation using Neo4j. These represent two fundamentally different paradigms for knowledge organization..."

### About API (RQ3)
> "We provide a unified JSON API that abstracts the underlying representation format, enabling seamless integration with downstream RAG systems regardless of whether schema-based or graph-based representations are used internally."

### About Deployment
> "The system is designed to be deployment-ready and can be easily deployed to cloud platforms. For demonstration purposes, we deployed the API to Render, enabling real-time access for evaluation."

---

## Key Takeaways Summary

1. **RQ1 is not the core**: LLM extraction is sufficient; focus on transformation
2. **RQ2-3 are core contributions**: Compare different representations' performance in moderation scenarios
3. **Unified knowledge base design**: One KB, use metadata to distinguish domains
4. **Start API simple**: JSON files â†’ Python module â†’ REST API
5. **Evaluate three dimensions**: Traceability, Reasoning, Maintenance
6. **Cost can be zero**: Local development + Render free tier

---

*Document compiled: 2026-02-05*
